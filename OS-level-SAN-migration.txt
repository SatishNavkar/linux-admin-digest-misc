
AsPerGuidance == SAN migration (possible vendors involved : EMC NetApp 3PAR)

--- Double-check this plan ---- Try on dummy ---

** Collect all pre-requisites

   vxprint -htg <DG>
   vxconfigbackup -l <mount>   ## check /etc/cbr/vx/bk/
   systool -c fc_host -v | grep -i port

** Scan all new "disks"

   echo "- - -" > /sys/class/scsi_host/hostX/scan
   vxdctl enable; vxdisk scandisks

** Take new "disks" in VxVM control

   vxdisksetup -i <new-disk's> ...

** Add new disks to appropriate DG

   vxdg -g <DG> adddisk <disk-name1>=<new-disk01> <disk-name2>=<new-disk02>

** Mirror old disk with new disk .. to preserve data from disk to be removed ==== need to find exact step here

   vxmirror -bg <DG> volume <VOL> <old-disk01> <new-disk01> ..  --- check syntax ??
   or
   vxmirror -g <DG> -a <new-disk01> <new-disk02> ..
   or
   vxmirror -g <DG> <old-disk01>  <new-disk01>
   or
   vxassist -b -g <DG> mirror <VOL> layout=nostripe alloc=disk01,..2,..3,..
   or
   vxmirror -bg <DG> volume alloc emcx  --- ??

** Monitor mirroring process

   vxtask list

** Break mirror after sync is completed

   vxplex -g <DG> -o rm dis <old-disk01> ..

** Remove disks from DG and remove from VxVM control

   vxdisk rmdisk <old-disk01> ...   ## check .. needs reclaim steps ?
   vxdiskunsetup -c <old-disk01> ..  ## check if really required

** Rescan after disks are removed from storage-end

   echo 1 > /sys/class/fc_host
    or
   echo "- - -" > /sys/class/scsi_host/hostX/scan


===== Additional .. if disk is showing failed .. check if really failed or mark failing flag off

   vxedit -g <DG> set failing=off <disk01>

* Another sample .. setting owner of raw disks .. in case of sybase mostly

   vxedit -g <DG> set user=sybase group=sybase <raw-vol>

* Setting clone flag off or on

   vxdisk set <disk-name> clone=<off/on>
   ++ # vxdisk updateudid <disk-name>


-----------
As-per-Exp == VMserver had limitation of 4 RDM disks only and expansion was requested. Hence decided to replace old 2 disks (of lesser size) with new 2 disks of bigger size.

[1] "sdx" disks corrosponding to "emc/vxvm" disks (mapping)
dzdo df -h;dzdo vxdmpadm getsubpaths;lsblk | grep -i sdx | grep -v part;lsblk | grep -i sdx | grep -v part;lsblk | grep -i sdx | grep -v part;

[2] Check existing volumes and plex's using == vxprint -htg <DG>

[3] VM needs to shutdown (if VCS then offline it) to add RDM by Vmware team. After adding RDM and server is up, we have rescan scsi buses for new RDM disk.

[4] Initialize newly assigned disk(1) under vxvm. vxdisksetup -i emc0_xxx

[5] Add new disk(1) to existing DG (with proper disk label).

[6] Mirror volumes and check status using vxtask list.

    # nohup vxassist -g <DG> mirror <VOL> &

[7] After mirroring check "vxprint -hvt <DG>"

[8] Now 'detach' (not to disassociate now) all old plex that you can see in "vxprint -hvt <DG>" output, that are created from old disks. (Be careful)
    DETACH plex means = a write to vol is not reflected to plex. A read req from vol will never be satisfied from detached plex.

    # vxplex -g <DG> det <VOL-01-plex-dcl-plex as well>

[9] vxprint .. old plex are detached and volumes still enabled using new mirrored plex.

[10] Now ask DB/App team to do health checks, if everything is fine - then remove old plex's and sub-disk's from DG and finally remove old disk from VxVM.

  if any issue then re-attach old plexs
  # vxplex -g <DG> att <VOL>

  to dis-associate plexs (data loss) :
  # vxplex -g <DG> -o rm dis <VOL-and-dcl as well>

  remove disk from DG :
  # vxdg -g <DG> rmdisk <disk-label>  ---- here might need vxdisk reclaim as well

  remove disk from vxvm
  # vxdiskunsetup -C <disk>

[11] Resize volume with additional capacity of newly added disk :

    vxassist -g <DG> maxgrow <VOL>
    vxassist -g <DG> growto <VOL> <size shown in above o/p>
    vxresize -g <DG> <VOL> <size shown in above o/p>
    df -h --- check new size
